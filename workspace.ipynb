{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import mmap\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from numba.typed import Dict, List\n",
    "from numba import types\n",
    "from transformers import (\n",
    "    BertTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/datacosmos/local/User/baoht/onesparse2/hybridsearch/bm25/index/marco/index_dist.json\") as f:\n",
    "    bm25_en_dist = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_en_dist = dict(sorted(bm25_en_dist.items(), key=lambda item: int(item[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_temp = {}\n",
    "for k, v in bm25_en_dist.items():\n",
    "    dict_temp[int(k)] = v\n",
    "dict_temp = dict(sorted(dict_temp.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnt\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_lists:\n\u001b[0;32m---> 13\u001b[0m     lens \u001b[38;5;241m=\u001b[39m \u001b[43mcount_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lens\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m lines\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(file, lens))\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mcount_lines\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m         cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cnt\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "filename = \"/datacosmos/local/User/baoht/onesparse2/marcov2/data/collection_train\"\n",
    "file_lists = [os.path.join(filename, f'part-{i}') for i in range(20)]\n",
    "\n",
    "def count_lines(filename):\n",
    "    cnt = 0 \n",
    "    with open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "            \n",
    "for file in file_lists:\n",
    "    lens = count_lines(file)\n",
    "    total += lens\n",
    "    print(\"File {} has {} lines\".format(file, lens))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weight = torch.ones(105879)\n",
    "weight[:173]  = 0\n",
    "with open(\"models/sparse_vocab_weight.tensor\", \"wb\") as f:\n",
    "    torch.save(weight, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import pickle\n",
    "\n",
    "base_dir = \"/datacosmos/User/baoht/onesparse2/marcov2/\"\n",
    "train_gt_path = \"data/qrels_train.tsv\"\n",
    "gt_dict = {}\n",
    "with open(base_dir + train_gt_path, 'r') as f:\n",
    "    for l in f:\n",
    "        try:\n",
    "            l = l.strip().split('\\t')\n",
    "            qid = int(l[0])\n",
    "            if qid in gt_dict:\n",
    "                pass\n",
    "            else:\n",
    "                gt_dict[qid] = []\n",
    "            gt_dict[qid].append(int(l[2]))\n",
    "        except:\n",
    "            raise IOError('\\\"%s\\\" is not valid format' % l)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9206475it [00:10, 858297.22it/s]\n"
     ]
    }
   ],
   "source": [
    "train_questions = {}\n",
    "with open(base_dir + \"data/queries_train.tsv\", \"r\", encoding=\"utf-8\") as ifile:\n",
    "    for line in tqdm(ifile):\n",
    "        line = line.strip().split('\\t')\n",
    "        id, text, language = line\n",
    "        train_questions[int(id)] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_train_path = base_dir + \"data/lmdb_data/\"\n",
    "doc_pool_env = lmdb.open(passages_train_path, subdir=os.path.isdir(passages_train_path), readonly=True, lock=False, readahead=False, meminit=False)\n",
    "passages_txn = doc_pool_env.begin(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = train_questions[278445]\n",
    "psg = pickle.loads(passages_txn.get(\"69399268\".encode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertTokenizerFast\n",
    ")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "tokenizer.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 13221, 20348, 30329, 79900, 102]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

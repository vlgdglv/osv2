{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import mmap\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from numba.typed import Dict, List\n",
    "from numba import types\n",
    "from transformers import (\n",
    "    BertTokenizer, AutoModelForMaskedLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/datacosmos/local/User/baoht/onesparse2/hybridsearch/bm25/index/marco/index_dist.json\") as f:\n",
    "    bm25_en_dist = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_en_dist = dict(sorted(bm25_en_dist.items(), key=lambda item: int(item[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_temp = {}\n",
    "for k, v in bm25_en_dist.items():\n",
    "    dict_temp[int(k)] = v\n",
    "dict_temp = dict(sorted(dict_temp.items(), key=lambda item: item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "filename = \"/datacosmos/local/User/baoht/onesparse2/marcov2/data/collection_train\"\n",
    "file_lists = [os.path.join(filename, f'part-{i}') for i in range(20)]\n",
    "\n",
    "def count_lines(filename):\n",
    "    cnt = 0 \n",
    "    with open(filename, 'rb') as f:\n",
    "        for line in f:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "            \n",
    "for file in file_lists:\n",
    "    lens = count_lines(file)\n",
    "    total += lens\n",
    "    print(\"File {} has {} lines\".format(file, lens))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "weight = torch.ones(105879)\n",
    "weight[:173]  = 0\n",
    "with open(\"models/sparse_vocab_weight.tensor\", \"wb\") as f:\n",
    "    torch.save(weight, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import pickle\n",
    "\n",
    "base_dir = \"/datacosmos/User/baoht/onesparse2/marcov2/\"\n",
    "train_gt_path = \"data/qrels_test.tsv\"\n",
    "gt_dict = {}\n",
    "with open(base_dir + train_gt_path, 'r') as f:\n",
    "    for l in f:\n",
    "        try:\n",
    "            l = l.strip().split('\\t')\n",
    "            qid = int(l[0])\n",
    "            if qid in gt_dict:\n",
    "                pass\n",
    "            else:\n",
    "                gt_dict[qid] = []\n",
    "            gt_dict[qid].append(int(l[2]))\n",
    "        except:\n",
    "            raise IOError('\\\"%s\\\" is not valid format' % l)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9206475it [00:10, 858297.22it/s]\n"
     ]
    }
   ],
   "source": [
    "train_questions = {}\n",
    "with open(base_dir + \"data/queries_train.tsv\", \"r\", encoding=\"utf-8\") as ifile:\n",
    "    for line in tqdm(ifile):\n",
    "        line = line.strip().split('\\t')\n",
    "        id, text, language = line\n",
    "        train_questions[int(id)] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_train_path = base_dir + \"data/training_data/test_lmdb_new\"\n",
    "doc_pool_env = lmdb.open(passages_train_path, subdir=os.path.isdir(passages_train_path), readonly=True, lock=False, readahead=False, meminit=False)\n",
    "passages_txn = doc_pool_env.begin(write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict[1]\n",
    "text = pickle.loads(passages_txn.get((\"185462934\").encode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'uin-\\ue000app\\ue001 的条件编译（\\ue000APP-PLUS\\ue001 、H5、MP-WEIXIN ...',\n",
       " 'uin-app 的条件编译（APP-PLUS 、H5、MP-WEIXIN ）_leoliyou的博客-CSDN博客#N#uin-app 的条件编译（APP-PLUS 、H5、MP-WEIXIN ）#N#leoliyou 于 2020-07-29 15:48:02 发布 6203 收藏 22#N#分类专栏： Uin-app#N#版权#N#Uin-app 专栏收录该内容#N#1 篇文章 0 订阅#N#订阅专栏#N#一、条件编译#N#条件编译是用特殊的注释作为标记，在编译时根据这些特殊的注释，将注释里面的代码编译到不同平台。#N#写法：以 #ifdef 或 #ifndef 加 %PLATFORM% 开头，以 #endif 结尾。#N#1 #ifdef：if defined 仅在某平台存在#N#2 #ifndef：if not defined 除了某平台均存在#N#3 %PLATFORM%：平台名称#N#条件编译写法 说明#N##ifdef APP-PLUS#N#需条件编译的代码 仅出现在 App 平台下的代码#N##endif#N##ifndef H5#N#需条件编译的代码 除了 H5 平台，其它平台均存在的代码#N##endif#N##ifdef H5 || MP-WEIXIN#N#需条件编译的代码 在 H5 平台或微信小程序平台存在的代码（这里只有||，不可能出现&&，因为没有交集）#N##endif#N#值 平台#N#APP-PLUS App#N#APP-PLUS-NVUE App nvue#N#H5 H5#N#MP-WEIXIN 微信小程序#N#MP-ALIPAY 支付宝小程序#N#MP-BAIDU 百度小程序#N#MP-TOUTIAO 字节跳动小程序#N#MP-QQ QQ小程序#N#MP-360 360小程序#N#MP 微信小程序/支付宝小程序/百度小程序/字节跳动小程序/QQ小程序/360小程序#N#quickapp-webview 快应用通用(包含联盟、华为)#N#quickapp-webview-union 快应用联盟#N#quickapp-webview-huawei 快应用华为#N#以上在兼容H5、app、小程序有着非常重要的作用，记录下以便大家查找#N#leoliyou#N#关注#N#9#N#0#N#22#N#专栏目录#N#uni-app 框架超详细新手入门#N#chen4565的博客#N#4020#N#什么是uni-app ?(介绍)#N#uni-app 是一个使用 Vue.js 开发跨平台应用 的 前端框架。#N#开发者通过编写 Vue.js 代码，uni-app 将其 编译 到iOS 、 Android 、 微信小程序等多个平台，保证其正确运行并达到优秀体验。#N#uni-app 继承自 Vue.js，提供了完整 的 Vue.js 开发体验。#N#uni-app 组件规范和扩展api与微信小程序基本相同。#N#有一定 Vue.js 和微信小程序开发经验 的 开发者可快速上手 uni-app ，开发出兼容多端 的 应用。#N#uni-app 提供了 条件编译#N#uni-app 1 、 app-plus 的 使用，#ifdef MP 只兼容小程序#N#weixin_42861240的博客#N#1万+#N#最近开始查看uni-app 的 一些项目，在pages.json里面发现 app-plus 。百度了下看见一些网友 的 解释是 app 跟 h5 端执行，小程序则不执行 （ 只测试过微信小程序，据说其他小程序也不执行 ） 。代码如下#N#{#N##TAB##TAB##TAB#\"path\": \"pages/index/index\",#N##TAB##TAB##TAB#\"style\": {#N##TAB##TAB##TAB##TAB#// #ifdef MP /* #TAB#微信小程序/支付宝小程序/百度小程序/头条...#N#uni app 微信支付宝小程序获取用户信息#N#俗人#N#724#N#我们也知道微信小程序前段时间更新了获取用户信息方法 （ 区别 ） ，今天介绍一下#N#uni-app 基础知识#N#m0_44973790的博客#N#402#N#1.uni-app 进行 条件编译 的 两种方法？小程序端和 H5 的 代表值是什么？#N##N##N#通过#ifdef 、 #idndef 的 方式 H5 ： H5 MP-WEIXIN: 微信小程序#N##N#2.uni-app 的 配置文件，入口文件，主组件，页面管理部分#N##N##N#page.json 配置文件#N#main.js 入口文件 App .vue 主组件#N#pages 页面管理部分#N##N#3.uni-app 上传文件时用到 的 API是什么？格式是什么#N##N##N#u#N#HB ui lderx 报错：nvue中不支持如下css。如全局或公共样式受影响，建议将告警样式写在ifndef APP-PLUS-NVUE 的 条件编译 中#N#约妲己吃火锅的博客#N#8421#N#之前在做 app 开发途中，更新软件版本之后发现 的 问题，报错:#N#nvue中不支持如下css。如全局或公共样式受影响，建议将告警样式写在ifndef APP-PLUS-NVUE 的 条件编译 中#N#报错如图：#N##N#坏处是 app 的 css样式不生效，但是好消息是，它不影响打包和运行。#N#去官方文档看了：#N#nvue页面 编译 模式差异#N#uni-app 深度改进了 weex，提供了2种 编译 模式，一种是常规 的 weex 组件模式，即编写<div>。另一种是 uni-app 组件模式，即编写<view>。后者更提供了#N#2021-09-25 uni app 的 条件编译 ，用于各种兼容 的 问题解决 （ 文图详解 ）#N#weixin_45721912的博客#N#240#N#1. 由于UNI APP 是一套代码多个平台都可以使用，所以就有些兼容问题要处理。有些代码小程序是没有问题，但是到了 H5 或者是 APP 就无法使用 的 问题。#N#2.这些问题呢，UNI APP 有一个API 的 条件编译 可以解决，就跟VUE 的 if else一样简单方便，可以解决大部分 的 兼容问题，接下来就给大家详细 的 列举一下 并告诉怎么使用。#N#3.使用方法和类型#N#3.1下面 的 样式是在js中使用#N##N##N##N#// #ifdef APP-PLUS 此代码只在 APP 中出现#N#// #endif#N##N##N##N##N#// #ifdef H5 此代码#N#（ 转 ） uni-app 开发中 的 #ifdef MP 是什么意思？ 条件编译#N#最新发布#N#weixin_44984136的博客#N#575#N#跨端兼容https://uni app .dcloud.io/platform#N##N##N##N#uni-app 已将常用 的 组件 、 JS API 封装到框架中，开发者按照 uni-app 规范开发即可保证多平台兼容，大部分业务均可直接满足。#N##N##N##N#但每个平台有自己 的 一些特性，因此会存在一些无法跨平台']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     BertTokenizerFast\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[43mquery\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertTokenizerFast\n",
    ")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "tokenizer.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"splade_index/warmup/shard_0/doc_ids_splade_index.bin.pkl\", \"rb\") as f:\n",
    "    doc_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['159555704',\n",
       " '196055807',\n",
       " '196055816',\n",
       " '206797042',\n",
       " '196055823',\n",
       " '120904217',\n",
       " '129494639',\n",
       " '178877602',\n",
       " '155262280',\n",
       " '144529003']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"splade_index/warmup/shard_0/doc_ids_splade_index.bin.pkl\", 'rb') as f:\n",
    "    doc_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9373\n"
     ]
    }
   ],
   "source": [
    "import lmdb\n",
    "import pickle\n",
    "\n",
    "base_dir = \"/datacosmos/User/baoht/onesparse2/marcov2/\"\n",
    "train_gt_path = \"data/qrels_test.tsv\"\n",
    "gt_dict = {}\n",
    "with open(base_dir + train_gt_path, 'r') as f:\n",
    "    for l in f:\n",
    "        try:\n",
    "            l = l.strip().split('\\t')\n",
    "            qid = int(l[0])\n",
    "            if qid in gt_dict:\n",
    "                pass\n",
    "            else:\n",
    "                gt_dict[qid] = []\n",
    "            gt_dict[qid].append(int(l[2]))\n",
    "        except:\n",
    "            raise IOError('\\\"%s\\\" is not valid format' % l)\n",
    "            \n",
    "gt_set = set()\n",
    "for k, v in gt_dict.items():\n",
    "    for pid in v:\n",
    "        gt_set.add(int(pid))\n",
    "print(len(gt_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1839\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for pid in doc_ids:\n",
    "    if int(pid) in gt_set:\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185462934]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = {\"text_id\": 1, \"text\": [10282, 35821, 10608, 108, 10481, 11526, 116, 39475, 59906, 19441, 39520, 11985, 15386, 14020, 51846, 69930, 23588, 85698, 12866, 24296, 21105, 26157, 78061, 17376, 53368, 82787, 22850, 13145, 49559, 56721, 58787, 20460], \"value\": [1.6295467615127563, 1.5897133350372314, 1.5630666017532349, 1.477690577507019, 1.35756516456604, 1.3565596342086792, 1.0506030321121216, 0.7076893448829651, 0.562402069568634, 0.5415972471237183, 0.5232481360435486, 0.48670902848243713, 0.4450363218784332, 0.4196864068508148, 0.409363716840744, 0.3910386264324188, 0.347847044467926, 0.31204116344451904, 0.31132611632347107, 0.3034266233444214, 0.24018500745296478, 0.20102573931217194, 0.18167029321193695, 0.151916041970253, 0.14095008373260498, 0.13414156436920166, 0.09761575609445572, 0.08244366943836212, 0.08244366943836212, 0.05046287178993225, 0.012615403160452843, 0.007782140746712685]}\n",
    "# {\"text_id\": 2, \"text\": [110, 50017, 122, 1596, 94405, 20187, 30645, 82787, 39520, 11937, 29562, 39475, 18612, 31725, 13409, 46983, 85698, 13121, 95009, 26157, 41256, 1651, 20098, 92229, 20910, 18465, 51846, 51448, 84099, 10399, 10889, 0], \"value\": [2.068641185760498, 1.4918500185012817, 1.4283655881881714, 1.1530905961990356, 1.0894558429718018, 0.8161250352859497, 0.6902132034301758, 0.6064908504486084, 0.6054252982139587, 0.5278670787811279, 0.5104349255561829, 0.48068851232528687, 0.472198486328125, 0.4374985992908478, 0.3757305145263672, 0.35677260160446167, 0.3325583338737488, 0.23247487843036652, 0.2018241435289383, 0.175953671336174, 0.1477125883102417, 0.1212492436170578, 0.10555581003427505, 0.10555581003427505, 0.04767346754670143, 0.034552380442619324, 0.034552380442619324, 0.0317181795835495, 0.025073640048503876, 0.023167060688138008, 0.0038986406289041042, 0.0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ptca/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertTokenizerFast\n",
    ")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##de app plus #f if + xml sql linux cpu pdf microsoft software gsm tracking twitter hilbert windows bmw facebook itunes bundestag administrative playboy iupac legislative standard fenster rpg layout boeing\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(qe[\"text\"], skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name =  \"google-bert/bert-base-multilingual-uncased\"\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0897, -0.0904, -0.0862,  ..., -0.1417, -0.1358, -0.1432])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_model.cls.state_dict()['predictions.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_zoo import BiEncoder\n",
    "from auxiliary import ModelConfig\n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"models/init_SimANS_ckpt_36k\",\n",
    "    vocab_size=105879\n",
    ")\n",
    "model = BiEncoder(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0897, -0.0904, -0.0862,  ..., -0.1417, -0.1358, -0.1432])\n",
      "tensor([-0.0897, -0.0904, -0.0862,  ..., -0.1417, -0.1358, -0.1432])\n"
     ]
    }
   ],
   "source": [
    "print(model.k_encoder.mlm_head.state_dict()['predictions.bias'])\n",
    "print(model.q_encoder.mlm_head.state_dict()['predictions.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
